-------------------LR: 0.001, hidden layer dim = 8, 100 epochs---------------------------------------

Run 1 after transformation of data:

Total MSE Loss by epoch 100: 70.22.

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 3.8947 hartree
S22-5: Error = 2.8275 hartree
S22by7-5-0.7: Error = 2.7615 hartree
S22by7-13-0.8: Error = 2.5122 hartree
S22by7-22-2.0: Error = 2.1811 hartree
S22by7-22-1.5: Error = 2.0839 hartree
S22by7-6-1.2: Error = 2.0411 hartree
S22by7-22-1.2: Error = 2.0380 hartree
S22-21: Error = 2.0149 hartree
S22-22: Error = 2.0068 hartree

Run 2 after transformation of data:

Total MSE Loss by epoch 100: 11.822115.

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 3.1195 hartree
S22by7-5-0.7: Error = 2.3228 hartree
S22-5: Error = 2.2768 hartree
S22by7-13-0.8: Error = 2.2481 hartree
S22by7-22-2.0: Error = 1.8086 hartree
S22by7-22-1.5: Error = 1.6299 hartree
S22by7-6-1.2: Error = 1.6056 hartree
S22by7-22-0.7: Error = 1.5990 hartree
S22-6: Error = 1.5691 hartree
S22by7-6-1.0: Error = 1.5681 hartree

Run 3 after transformation of data:

Total MSE Loss by epoch 100: 50.779744.

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 3.4310 hartree
S22-5: Error = 2.3799 hartree
S22by7-5-0.7: Error = 2.2406 hartree
S22by7-22-2.0: Error = 1.9337 hartree
S22by7-22-1.5: Error = 1.8145 hartree
S22by7-22-1.2: Error = 1.7200 hartree
S22by7-6-1.2: Error = 1.7171 hartree
S22by7-6-1.0: Error = 1.6350 hartree
S22-6: Error = 1.6340 hartree
S22by7-22-1.0: Error = 1.6283 hartree

Run 4 after tranformation of data:

Total MSE Loss by epoch 100: Approx 133.929901.

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 5.6081 hartree
S22-5: Error = 3.8973 hartree
S22by7-5-0.7: Error = 3.6585 hartree
S22by7-22-2.0: Error = 3.1999 hartree
S22by7-22-1.5: Error = 2.9493 hartree
S22by7-6-1.2: Error = 2.7886 hartree
S22by7-22-1.2: Error = 2.7669 hartree
S22-6: Error = 2.6429 hartree
S22by7-6-1.0: Error = 2.6420 hartree
S22-22: Error = 2.6162 hartree


Run 5:

Epoch 100: Total MSE Loss = 140.647890

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 5.9470 hartree
S22-5: Error = 3.9798 hartree
S22by7-5-0.7: Error = 3.5692 hartree
S22by7-22-2.0: Error = 3.5081 hartree
S22by7-22-1.5: Error = 3.0812 hartree
S22by7-6-1.2: Error = 2.7976 hartree
S22by7-22-1.2: Error = 2.7609 hartree
S22by7-6-1.0: Error = 2.5464 hartree
S22-6: Error = 2.5454 hartree
S22by7-22-1.0: Error = 2.5085 hartree


-------------------LR: 0.001, hidden layer dim = 16, 100 epochs---------------------------------------

Run 1:

Epoch 100: Total MSE Loss = 171.599010

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 6.1926 hartree
S22-5: Error = 4.4175 hartree
S22by7-5-0.7: Error = 4.2515 hartree
S22by7-13-0.8: Error = 3.5386 hartree
S22by7-22-2.0: Error = 3.4435 hartree
S22by7-22-1.5: Error = 3.2943 hartree
S22by7-6-1.2: Error = 3.2038 hartree
S22by7-22-1.2: Error = 3.1933 hartree
S22-21: Error = 3.1237 hartree
S22-6: Error = 3.1168 hartree


Run 2:

Epoch 100: Total MSE Loss = 116.887120

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 5.8092 hartree
S22by7-5-0.7: Error = 4.4555 hartree
S22-5: Error = 4.2676 hartree
S22by7-13-0.8: Error = 3.8089 hartree
S22by7-22-2.0: Error = 3.2314 hartree
S22by7-22-1.5: Error = 3.0878 hartree
S22by7-6-0.9: Error = 3.0823 hartree
S22by7-6-1.2: Error = 3.0634 hartree
S22by7-6-1.0: Error = 3.0573 hartree
S22-6: Error = 3.0564 hartree

Run 3:

Epoch 100: Total MSE Loss = 21.161371

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 2.5958 hartree
S22-5: Error = 1.8606 hartree
S22by7-5-0.7: Error = 1.7478 hartree
S22by7-22-2.0: Error = 1.5599 hartree
S22by7-13-0.8: Error = 1.4691 hartree
S22by7-22-1.5: Error = 1.3776 hartree
S22by7-6-1.2: Error = 1.3018 hartree
S22by7-22-1.2: Error = 1.2759 hartree
S22-6: Error = 1.2382 hartree
S22by7-6-1.0: Error = 1.2372 hartree

-------------------LR: 0.0001 (smaller than above), hidden layer dim = 8, 100 epochs------------------

Run 1:

Epoch 100: Total MSE Loss = 11.068267

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 1.9263 hartree
S22by7-15-1.5: Error = 1.2111 hartree
S22by7-22-2.0: Error = 0.9445 hartree
S22by7-22-1.5: Error = 0.5341 hartree
S22-12: Error = 0.5078 hartree
S22by7-3-0.7: Error = 0.5043 hartree
S22-5: Error = 0.4999 hartree
S22by7-4-0.7: Error = 0.4944 hartree
S22by7-22-0.7: Error = 0.4832 hartree
S22-11: Error = 0.4625 hartree

Run 2:
Epoch 100: Total MSE Loss = 316.328292
Top 10 largest prediction errors:
S22by7-13-0.8: Error = 8.5651 hartree
S22by7-15-1.5: Error = 8.4771 hartree
S22by7-22-2.0: Error = 6.0761 hartree
S22-5: Error = 4.1816 hartree
S22by7-22-1.5: Error = 3.9553 hartree
S22by7-3-0.7: Error = 2.4951 hartree
S22by7-4-0.7: Error = 2.4801 hartree
S22by7-6-1.2: Error = 2.4618 hartree
S22by7-22-1.2: Error = 2.2937 hartree
S22by7-9-0.7: Error = 2.1864 hartree

Run 3:

Epoch 100: Total MSE Loss = 2.891278

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 0.7627 hartree
S22by7-15-1.5: Error = 0.6973 hartree
S22by7-22-2.0: Error = 0.4998 hartree
S22-5: Error = 0.3388 hartree
S22by7-22-1.5: Error = 0.3251 hartree
S22by7-4-0.7: Error = 0.2940 hartree
S22by7-3-0.7: Error = 0.2886 hartree
S22by7-3-0.8: Error = 0.2195 hartree
S22by7-4-0.8: Error = 0.2078 hartree
S22by7-9-0.7: Error = 0.2061 hartree

Run 4:

Epoch 100: Total MSE Loss = 2.252895

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.6960 hartree
S22-5: Error = 0.4997 hartree
S22by7-5-0.7: Error = 0.4618 hartree
S22by7-13-0.8: Error = 0.3990 hartree
S22by7-22-2.0: Error = 0.3820 hartree
S22by7-22-1.5: Error = 0.3719 hartree
S22-21: Error = 0.3667 hartree
S22by7-6-1.2: Error = 0.3653 hartree
S22by7-22-1.2: Error = 0.3652 hartree
S22-22: Error = 0.3576 hartree

Run 5:

Epoch 100: Total MSE Loss = 4.118754

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 1.1703 hartree
S22by7-15-1.5: Error = 0.7706 hartree
S22by7-22-2.0: Error = 0.5979 hartree
S22by7-22-1.5: Error = 0.3412 hartree
S22-5: Error = 0.3258 hartree
S22-12: Error = 0.3104 hartree
S22by7-3-0.7: Error = 0.2901 hartree
S22by7-22-0.7: Error = 0.2842 hartree
S22-11: Error = 0.2814 hartree
S22by7-4-0.7: Error = 0.2813 hartree

-------------------LR: 0.0001 , hidden layer dim = 16, 100 epochs--------------------------------------

Run 1:

Epoch 100: Total MSE Loss = 15.858676

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 2.3432 hartree
S22by7-15-1.5: Error = 1.3775 hartree
S22by7-22-2.0: Error = 1.1191 hartree
S22-12: Error = 0.6906 hartree
S22-11: Error = 0.6334 hartree
S22by7-22-0.7: Error = 0.6144 hartree
S22by7-22-1.5: Error = 0.5893 hartree
S22-5: Error = 0.5608 hartree
S22by7-4-0.7: Error = 0.5404 hartree
S22by7-3-0.7: Error = 0.5258 hartree

Run 2:

Epoch 100: Total MSE Loss = 49.864769

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 4.0982 hartree
S22by7-15-1.5: Error = 2.4863 hartree
S22by7-22-2.0: Error = 1.9465 hartree
S22by7-3-0.7: Error = 1.1096 hartree
S22by7-4-0.7: Error = 1.0980 hartree
S22by7-22-1.5: Error = 1.0932 hartree
S22-12: Error = 1.0743 hartree
S22by7-22-0.7: Error = 1.0513 hartree
S22-5: Error = 1.0068 hartree
S22-11: Error = 0.9820 hartree

Run 3:

Epoch 100: Total MSE Loss = 17.333630

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 2.4190 hartree
S22by7-15-1.5: Error = 1.4307 hartree
S22by7-22-2.0: Error = 1.1173 hartree
S22by7-4-0.7: Error = 0.6632 hartree
S22by7-3-0.7: Error = 0.6518 hartree
S22by7-22-1.5: Error = 0.6299 hartree
S22by7-22-0.7: Error = 0.6191 hartree
S22-12: Error = 0.6175 hartree
S22-5: Error = 0.5750 hartree
S22-11: Error = 0.5676 hartree

Run 4:

Epoch 100: Total MSE Loss = 15.533665

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 2.1951 hartree
S22by7-15-1.5: Error = 1.3901 hartree
S22by7-22-2.0: Error = 1.0682 hartree
S22by7-4-0.7: Error = 0.7104 hartree
S22by7-3-0.7: Error = 0.7059 hartree
S22by7-22-1.5: Error = 0.6198 hartree
S22by7-3-0.8: Error = 0.5860 hartree
S22by7-22-0.7: Error = 0.5657 hartree
S22-5: Error = 0.5634 hartree
S22by7-4-0.8: Error = 0.5620 hartree

-------------------LR: 0.001 , hidden layer dim = 4, 100 epochs---------------------------
Run 1:

Epoch 100: Total MSE Loss = 38.006867

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 3.0972 hartree
S22-5: Error = 2.0230 hartree
S22by7-22-2.0: Error = 1.8413 hartree
S22by7-5-0.7: Error = 1.6971 hartree
S22by7-22-1.5: Error = 1.5943 hartree
S22by7-6-1.2: Error = 1.4188 hartree
S22by7-22-1.2: Error = 1.4015 hartree
S22-6: Error = 1.2560 hartree
S22by7-6-1.0: Error = 1.2550 hartree
S22-22: Error = 1.2438 hartree

Run 2:
Epoch 100: Total MSE Loss = 85.692794

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 4.6487 hartree
S22-5: Error = 3.1415 hartree
S22by7-5-0.7: Error = 2.8728 hartree
S22by7-22-2.0: Error = 2.7217 hartree
S22by7-22-1.5: Error = 2.4170 hartree
S22by7-6-1.2: Error = 2.2168 hartree
S22by7-22-1.2: Error = 2.1899 hartree
S22by7-6-1.0: Error = 2.0410 hartree
S22-6: Error = 2.0400 hartree
S22by7-22-1.0: Error = 2.0106 hartree


-------------------LR: 0.0001 , hidden layer dim = 16, 200 epochs(prev. 100)---------------------------

Epoch 200: Total MSE Loss = 0.385678

Top 10 largest prediction errors:
S22by7-13-0.8: Error = 0.4317 hartree
S22-11: Error = 0.2187 hartree
S22-12: Error = 0.2004 hartree
S22-21: Error = 0.1210 hartree
S22by7-22-0.8: Error = 0.1159 hartree
S22by7-22-0.7: Error = 0.1121 hartree
S22by7-22-0.9: Error = 0.1020 hartree
S22by7-9-0.7: Error = 0.0994 hartree
S22by7-22-1.0: Error = 0.0990 hartree
S22-22: Error = 0.0981 hartree

Run 2:
Epoch 200: Total MSE Loss = 1.049732

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.4936 hartree
S22by7-22-2.0: Error = 0.4647 hartree
S22by7-13-0.8: Error = 0.4070 hartree
S22-5: Error = 0.3921 hartree
S22by7-5-0.7: Error = 0.3842 hartree
S22by7-22-1.5: Error = 0.2953 hartree
S22by7-22-0.9: Error = 0.2703 hartree
S22-22: Error = 0.2683 hartree
S22by7-22-1.0: Error = 0.2674 hartree
S22by7-22-1.2: Error = 0.2587 hartree

Run 3:

Epoch 200: Total MSE Loss = 0.197562

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.2521 hartree
S22by7-5-0.7: Error = 0.2250 hartree
S22-5: Error = 0.1712 hartree
S22by7-22-2.0: Error = 0.1485 hartree
S22by7-22-1.5: Error = 0.1309 hartree
S22by7-6-0.9: Error = 0.1244 hartree
S22by7-6-1.2: Error = 0.1203 hartree
S22by7-22-1.2: Error = 0.1184 hartree
S22by7-6-1.0: Error = 0.1163 hartree
S22-6: Error = 0.1153 hartree
-------------------LR: 0.0001 , hidden layer dim = 8, 200 epochs---------------------------
Run 1:
Epoch 200: Total MSE Loss = 0.798595
The average loss per system by the last epoch was: 0.10118498450223237

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.3327 hartree
S22by7-5-0.7: Error = 0.2967 hartree
S22-5: Error = 0.2681 hartree
S22by7-6-0.9: Error = 0.1975 hartree
S22by7-22-2.0: Error = 0.1954 hartree
S22-6: Error = 0.1897 hartree
S22by7-6-1.0: Error = 0.1887 hartree
S22by7-6-1.2: Error = 0.1754 hartree
S22by7-22-1.5: Error = 0.1724 hartree
S22by7-22-0.7: Error = 0.1721 hartree

Run 2:
Epoch 200: Total MSE Loss = 2.781308
The average loss per system by the last epoch was: 0.18883273330170547

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.7683 hartree
S22-5: Error = 0.5579 hartree
S22by7-5-0.7: Error = 0.5248 hartree
S22by7-13-0.8: Error = 0.4903 hartree
S22-21: Error = 0.4179 hartree
S22by7-22-2.0: Error = 0.4172 hartree
S22by7-22-1.5: Error = 0.4125 hartree
S22by7-22-1.2: Error = 0.4103 hartree
S22by7-6-1.2: Error = 0.4098 hartree
S22-22: Error = 0.4057 hartree


-------------------LR: 0.0001 , hidden layer dim = 8, 300 epochs---------------------------
Run 1:
Epoch 300: Total MSE Loss = 0.935759

Top 10 largest prediction errors:
S22by7-15-1.5: Error = 0.4596 hartree




-------------------LR: 0.0001 , hidden layer dim = 8, 200 epochs, 287 samples---------------------------


Epoch 200: Total MSE Loss = 0.452643
The average loss per system by the last epoch was: 0.0397 (lowest so far)

Top 10 largest prediction errors:
S22by7-5-0.7: Error = 0.0702 hartree
SSI-095TYR-106TYR-1: Error = 0.0664 hartree
S22by7-22-2.0: Error = 0.0645 hartree
S22by7-4-0.7: Error = 0.0537 hartree
UBQ-glu18-asp21-C: Error = 0.0439 hartree
S22by7-3-0.7: Error = 0.0422 hartree
UBQ-pro19-ser57: Error = 0.0421 hartree
S22by7-13-0.8: Error = 0.0408 hartree
S22by7-15-1.5: Error = 0.0369 hartree
SSI-031PHE-095TYR-1: Error = 0.0313 hartree


-------------------LR: 0.001 , hidden layer dim = 8, 100 epochs, 287 samples---------------------------


Epoch 100: Total MSE Loss = 41.672124
The average loss per system by the last epoch was: 0.3810499139213059

Top 10 largest prediction errors:
S22by7-22-2.0: Error = 0.6805 hartree
SSI-095TYR-106TYR-1: Error = 0.6744 hartree
S22by7-13-0.8: Error = 0.6239 hartree
S22by7-5-0.7: Error = 0.5414 hartree
UBQ-glu18-asp21-C: Error = 0.5315 hartree
UBQ-pro19-ser57: Error = 0.4978 hartree
S22by7-15-1.5: Error = 0.4949 hartree
NBC1-BzBz_PD32-0.2: Error = 0.4739 hartree
S22-5: Error = 0.4501 hartree
UBQ-asp21-leu56: Error = 0.4348 hartree







-------------------LR: 0.0001 , hidden layer dim = 16, 200 epochs, 329 samples---------------------------

Epoch 200: Total MSE Loss = 0.198990
The average loss per system by the last epoch was: 0.02459334923 (lowest so far)

Top 10 largest prediction errors:
UBQ-thr22-thr55-big: Error = 0.0527 hartree (most pairs out of any file ~800)
ACHC-AC-3.4__0.4___: Error = 0.0478 hartree
ACHC-AC-3.4__0.8___: Error = 0.0476 hartree
ACHC-AC-3.2_____: Error = 0.0475 hartree
ACHC-AC-3.4__1.2___: Error = 0.0473 hartree
ACHC-AC-3.4__n0.4___: Error = 0.0471 hartree
ACHC-AC-3.4__n0.8___: Error = 0.0469 hartree
ACHC-AC-3.4___30__: Error = 0.0467 hartree
ACHC-AC-3.4_n0.4____: Error = 0.0466 hartree
ACHC-AC-3.4_0.4____: Error = 0.0460 hartree

-------------------LR: 0.0001 , hidden layer dim = 8, 200 epochs, 450 samples---------------------------

Epoch 200: Total MSE Loss = 0.146206
The average RMSE loss per system by the last epoch was: 0.018025072964822024 (lowest so far)

Top 10 largest prediction errors:
JSCH-15: Error = 0.0295 hartree
JSCH-40: Error = 0.0283 hartree
JSCH-19: Error = 0.0275 hartree
JSCH-55: Error = 0.0272 hartree
JSCH-45: Error = 0.0270 hartree
JSCH-44: Error = 0.0269 hartree
JSCH-23: Error = 0.0262 hartree
S22by7-13-0.8: Error = 0.0258 hartree
JSCH-43: Error = 0.0249 hartree
JSCH-25: Error = 0.0238 hartree

-------------------LR: 0.0001 , hidden layer dim = 16, 200 epochs, 450 samples---------------------------

Epoch 200: Total MSE Loss = 1.423568
The average RMSE loss per system by the last epoch was: 0.056244857801452816

Top 10 largest prediction errors:
JSCH-44: Error = 0.0776 hartree
JSCH-19: Error = 0.0762 hartree
S22by7-5-0.7: Error = 0.0705 hartree
JSCH-45: Error = 0.0667 hartree
JSCH-25: Error = 0.0649 hartree
UBQ-thr22-thr55-big: Error = 0.0634 hartree
JSCH-55: Error = 0.0616 hartree
JSCH-52: Error = 0.0606 hartree
JSCH-15: Error = 0.0602 hartree
JSCH-40: Error = 0.0580 hartree











Worst performing file is S22by7-15-1.5 on all modalities. Features of this file:

600 pairs ---> Higher than average (highest?)
Lowest Sij: 1.3e-7
Highest Sij: Around 1.5e-3
SAPT: -0.0024801508060000002 (not out of the norm)
Undamped: -0.0022067133000000035 (doesn't seem out of the norm)

S22by7-5-0.7: Error = 0.2250 hartree (441 pairs)
S22by7-22-2.0 (325 pairs)
S22by7-13-0.8(441 pairs)
S22-5: 441 pairs

